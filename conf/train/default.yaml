# Training configuration
epochs: 3
batch_size: 8
gradient_accumulation_steps: 1

# Optimizer
learning_rate: 2.0e-5
weight_decay: 0.01
adam_epsilon: 1.0e-8

# Scheduler
warmup_steps: 100
lr_scheduler_type: linear  # linear, cosine, constant

# Gradient clipping
max_grad_norm: 1.0

# Mixed precision
mixed_precision: no  # no, fp16, bf16

# Checkpointing
save_steps: 500
save_total_limit: 3
save_best: true
metric_for_best_model: eval_loss
greater_is_better: false

# Early stopping
early_stopping_patience: 3
early_stopping_threshold: 0.001
