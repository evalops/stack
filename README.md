# Transformers Stack

A cohesive Transformers stack built on PyTorch with uv for reproducible dependency management. This stack covers model definition, data loading, training/inference infrastructure, optimization/quantization, evaluation, and environment management.

## Core Components

| Layer | Packages & Rationale | Stable Version(s) |
|-------|---------------------|-------------------|
| **Base framework** | `torch` provides tensors, autograd and GPU support | `torch==2.8.0` |
| **Model definitions** | `transformers[torch]` supplies ~100 model architectures plus ready‑made pipelines | `transformers[torch]==4.56.2` |
| **Datasets & evaluation** | `datasets` offers fast dataset loading & streaming; `evaluate` and `scikit‑learn` provide metrics and classical ML utilities | `datasets==4.1.1`, `evaluate>=0.4.1`, `scikit-learn>=1.3` |
| **Environment management** | `uv` acts as a drop‑in replacement for pip/pip-tools with ultra-fast installation | `uv` (installed via script or pip) |

## Performance & Fine-tuning Extensions

| Purpose | Package(s) | Notes |
|---------|-----------|-------|
| **Multi‑GPU & distributed** | `accelerate==1.10.1` | Simplifies multi‑GPU, multi‑node and mixed‑precision training |
| **Parameter‑efficient fine‑tuning** | `peft==0.17.1` | Provides LoRA/P‑Tuning/etc. |
| **Quantization & 8‑bit ops** | `bitsandbytes==0.47.0` | Adds 8‑bit optimizers and int8 matmul; requires CUDA |
| **Memory‑efficient attention** | `flash-attn==2.8.3`, `xformers==0.0.32.post2` | FlashAttention and alternative efficient attention kernels |
| **High‑throughput inference** | `vllm==0.10.2` | Serves large‑language models with continuous batching |
| **Logging & monitoring** | `wandb` or `mlflow` | For experiment tracking (add as needed) |

## Installation

### 1. Install uv

```bash
curl -LsSf https://astral.sh/uv/install.sh | sh
```

### 2. Create & activate a virtual environment

```bash
uv venv --python=python3.11 .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate
```

### 3. Lock exact versions

```bash
uv pip compile pyproject.toml -o requirements.txt
```

### 4. Sync the environment with the lockfile

```bash
uv pip sync requirements.txt
```

`uv` ensures that your environment exactly matches the locked versions—packages not in `requirements.txt` are removed—so you avoid "dependency drift."

## Example Usage

### Train a model with LoRA and evaluate

```python
from datasets import load_dataset
from transformers import AutoModelForSequenceClassification, AutoTokenizer
from peft import LoraConfig, get_peft_model
import torch
from accelerate import Accelerator

acc = Accelerator()
ds = load_dataset("imdb", split="train[:1%]")  # small subset for demo

tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
enc = ds.map(lambda ex: tokenizer(ex["text"], truncation=True, padding=True), batched=True)
model = AutoModelForSequenceClassification.from_pretrained("bert-base-uncased")
model = get_peft_model(model, LoraConfig(r=8, lora_alpha=16))

model, optimizer, _, dataloader = acc.prepare(
    model,
    torch.optim.Adam(model.parameters(), lr=2e-5),
    None,
    torch.utils.data.DataLoader(enc.with_format("torch"), batch_size=8, shuffle=True),
)
model.train()
for batch in dataloader:
    with acc.accumulate(model):
        outputs = model(**{k: batch[k] for k in ["input_ids", "attention_mask"]}, labels=batch["label"])
        loss = outputs.loss
        acc.backward(loss)
        optimizer.step()
        optimizer.zero_grad()
```

This example shows the interaction between `datasets`, `transformers`, `peft`, and `accelerate`. Swap in `bitsandbytes` optimizers or `flash-attn` kernels as your hardware allows.

## Important Notes & Hazards

### CUDA-only modules

**⚠️ bitsandbytes, flash-attn and xformers require NVIDIA GPUs** and won't work on Apple Silicon or CPU‑only setups. If you're on macOS, omit them or use CPU/Metal‐accelerated alternatives (e.g., skip `bitsandbytes` and rely on full‑precision training).

### vLLM vs. Transformers inference

The Hugging Face `pipeline` API is fine for small tests, but for high‑throughput evaluation or serving you'll want `vllm`, which uses continuous batching. Ensure your GPU has enough memory.

### Stay pinned

Periodically check for new releases, update your `pyproject.toml` versions, run `uv pip compile` again, and sync. This keeps your stack consistent while still benefiting from improvements.

## Project Structure

```
stack/
├── pyproject.toml          # Project configuration and dependencies
├── README.md               # This file
├── examples/               # Example scripts
│   └── train_lora.py      # LoRA fine-tuning example
├── requirements.txt        # Locked dependencies (generated by uv)
└── .venv/                 # Virtual environment (created by uv)
```

## Development

Install development dependencies:

```bash
uv pip install -e ".[dev]"
```

This includes:
- `ruff` - Fast Python linter
- `black` - Code formatter
- `pytest` - Testing framework
- `ipykernel` - Jupyter notebook support

## License

MIT

## Contributing

Contributions are welcome! Please open an issue or pull request on GitHub.

## Resources

- [PyTorch Documentation](https://pytorch.org/docs/)
- [Hugging Face Transformers](https://huggingface.co/docs/transformers/)
- [uv Documentation](https://docs.astral.sh/uv/)
- [PEFT Documentation](https://huggingface.co/docs/peft/)
- [Accelerate Documentation](https://huggingface.co/docs/accelerate/)
